import os
import sys
import time
import datetime

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞PythonË∑ØÂæÑ
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

from nets.unet_training import CE_Loss, Dice_loss, Focal_Loss
from utils.utils import get_lr
from utils.utils_metrics import f_score


def calculate_fps(model, device, input_shape, batch_size=1, test_iterations=100):
    """
    Áã¨Á´ãÁöÑFPSÊµãËØïÂáΩÊï∞ - Âè™Âú®ËÆ≠ÁªÉÂÆåÊàêÂêéË∞ÉÁî®
    """
    print("üöÄ ÂºÄÂßãFPSÊÄßËÉΩÊµãËØï...")
    model.eval()
    
    # ÂàõÂª∫ÊµãËØïËæìÂÖ•
    test_input = torch.randn(batch_size, 3, input_shape[0], input_shape[1]).to(device)
    
    # È¢ÑÁÉ≠GPU
    print("‚è≥ GPUÈ¢ÑÁÉ≠‰∏≠...")
    with torch.no_grad():
        for _ in range(20):  # Â¢ûÂä†È¢ÑÁÉ≠Ê¨°Êï∞
            _ = model(test_input)
    
    # ÂêåÊ≠•GPU
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    print(f"üìä ÂºÄÂßã{test_iterations}Ê¨°Êé®ÁêÜÊµãËØï...")
    # ÂºÄÂßãÊ≠£ÂºèÊµãËØï
    time_list = []
    with torch.no_grad():
        for i in range(test_iterations):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            start_time = time.time()
            _ = model(test_input)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.time()
            time_list.append(end_time - start_time)
            
            # ÊòæÁ§∫ËøõÂ∫¶
            if (i + 1) % 20 == 0:
                print(f"  ÂÆåÊàê {i + 1}/{test_iterations} Ê¨°ÊµãËØï...")
    
    # ËÆ°ÁÆóFPSÁªüËÆ°
    time_array = np.array(time_list)
    avg_time = np.mean(time_array)
    min_time = np.min(time_array)
    max_time = np.max(time_array)
    std_time = np.std(time_array)
    
    fps_avg = batch_size / avg_time
    fps_max = batch_size / min_time
    fps_min = batch_size / max_time
    
    print(f"‚ö° FPSÊµãËØïÁªìÊûú:")
    print(f"   Âπ≥ÂùáFPS: {fps_avg:.2f}")
    print(f"   ÊúÄÂ§ßFPS: {fps_max:.2f}")
    print(f"   ÊúÄÂ∞èFPS: {fps_min:.2f}")
    print(f"   Ê†áÂáÜÂ∑Æ: {std_time*1000:.2f}ms")
    print(f"   Âπ≥ÂùáÊé®ÁêÜÊó∂Èó¥: {avg_time*1000:.2f}ms")
    
    model.train()  # ÊÅ¢Â§çËÆ≠ÁªÉÊ®°Âºè
    return fps_avg, fps_max, fps_min, avg_time

def test_model_fps(model, device, input_shape, log_dir, model_name="model", test_iterations=100):
    """
    Áã¨Á´ãÁöÑFPSÊµãËØïÂáΩÊï∞ÔºåÁªìÊûú‰øùÂ≠òÂà∞ÊåáÂÆöÁöÑlogÁõÆÂΩï
    Args:
        model: ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã
        device: ËÆ°ÁÆóËÆæÂ§á
        input_shape: ËæìÂÖ•ÂõæÂÉèÂ∞∫ÂØ∏ [H, W]
        log_dir: Êó•Âøó‰øùÂ≠òÁõÆÂΩï
        model_name: Ê®°ÂûãÂêçÁß∞
        test_iterations: ÊµãËØïËø≠‰ª£Ê¨°Êï∞
    """
    print("\n" + "="*60)
    print("üöÄ ÂºÄÂßãÊ®°ÂûãFPSÊÄßËÉΩÊµãËØï")
    print("="*60)
    
    model.eval()
    
    # ÊµãËØï‰∏çÂêåbatch_size
    batch_sizes = [1, 2, 4, 8]
    fps_results = {}
    test_results = []
    
    for batch_size in batch_sizes:
        print(f"\nüß™ ÊµãËØï Batch Size = {batch_size}")
        
        try:
            # ÂàõÂª∫ÊµãËØïËæìÂÖ•
            test_input = torch.randn(batch_size, 3, input_shape[0], input_shape[1]).to(device)
            
            # È¢ÑÁÉ≠GPU
            print("‚è≥ GPUÈ¢ÑÁÉ≠‰∏≠...")
            with torch.no_grad():
                for _ in range(20):
                    _ = model(test_input)
            
            # ÂêåÊ≠•GPU
            if device.type == 'cuda':
                torch.cuda.synchronize()
                # Ê£ÄÊü•ÊòæÂ≠ò‰ΩøÁî®
                memory_allocated = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB
                memory_reserved = torch.cuda.memory_reserved(device) / 1024 / 1024   # MB
                print(f"üìä ÊòæÂ≠ò‰ΩøÁî®: {memory_allocated:.1f}MB / {memory_reserved:.1f}MB")
            
            print(f"üöÄ ÂºÄÂßã{test_iterations}Ê¨°Êé®ÁêÜÊµãËØï...")
            time_list = []
            
            # ËøõË°åFPSÊµãËØï
            with torch.no_grad():
                for i in range(test_iterations):
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    start_time = time.time()
                    _ = model(test_input)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    time_list.append(end_time - start_time)
                    
                    # ÊòæÁ§∫ËøõÂ∫¶
                    if (i + 1) % (test_iterations // 4) == 0:
                        print(f"  ËøõÂ∫¶: {i + 1}/{test_iterations}")
            
            # ËÆ°ÁÆóÁªüËÆ°Êï∞ÊçÆ
            time_array = np.array(time_list)
            avg_time = np.mean(time_array)
            min_time = np.min(time_array)
            max_time = np.max(time_array)
            std_time = np.std(time_array)
            
            fps_avg = batch_size / avg_time
            fps_max = batch_size / min_time
            fps_min = batch_size / max_time
            
            result = {
                'batch_size': batch_size,
                'fps_avg': fps_avg,
                'fps_max': fps_max,
                'fps_min': fps_min,
                'avg_time_ms': avg_time * 1000,
                'min_time_ms': min_time * 1000,
                'max_time_ms': max_time * 1000,
                'std_time_ms': std_time * 1000,
                'memory_mb': memory_allocated if device.type == 'cuda' else 0
            }
            
            fps_results[batch_size] = result
            test_results.append(result)
            
            print(f"‚úÖ ÁªìÊûú:")
            print(f"   Âπ≥ÂùáFPS: {fps_avg:.2f}")
            print(f"   ÊúÄÂ§ßFPS: {fps_max:.2f}")
            print(f"   ÊúÄÂ∞èFPS: {fps_min:.2f}")
            print(f"   Âπ≥ÂùáÊé®ÁêÜÊó∂Èó¥: {avg_time*1000:.2f}ms")
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"‚ùå Batch Size {batch_size} ÊòæÂ≠ò‰∏çË∂≥: {str(e)}")
            else:
                print(f"‚ùå Batch Size {batch_size} ÊµãËØïÂ§±Ë¥•: {str(e)}")
            fps_results[batch_size] = None
    
    # ‰øùÂ≠òÁªìÊûúÂà∞logÁõÆÂΩï
    save_fps_results(test_results, log_dir, model_name, input_shape, device, test_iterations)
    
    model.train()  # ÊÅ¢Â§çËÆ≠ÁªÉÊ®°Âºè
    return fps_results

def save_fps_results(test_results, log_dir, model_name, input_shape, device, test_iterations):
    """‰øùÂ≠òFPSÊµãËØïÁªìÊûúÂà∞logÁõÆÂΩï"""
    
    # Á°Æ‰øùlogÁõÆÂΩïÂ≠òÂú®
    os.makedirs(log_dir, exist_ok=True)
    
    # ÁîüÊàêÊó∂Èó¥Êà≥
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ‰øùÂ≠òËØ¶ÁªÜÁöÑÊñáÊú¨Êä•Âëä
    fps_report_path = os.path.join(log_dir, f"fps_test_report_{timestamp}.txt")
    with open(fps_report_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("FPSÊÄßËÉΩÊµãËØïÊä•Âëä\n")
        f.write("="*60 + "\n\n")
        
        f.write(f"ÊµãËØïÊó∂Èó¥: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Ê®°ÂûãÂêçÁß∞: {model_name}\n")
        f.write(f"ËæìÂÖ•Â∞∫ÂØ∏: {input_shape}\n")
        f.write(f"ËÆ°ÁÆóËÆæÂ§á: {device}\n")
        f.write(f"ÊµãËØïÊ¨°Êï∞: {test_iterations}\n\n")
        
        f.write("ÊµãËØïÁªìÊûú:\n")
        f.write("-" * 40 + "\n")
        
        for result in test_results:
            f.write(f"\nBatch Size {result['batch_size']}:\n")
            f.write(f"  Âπ≥ÂùáFPS: {result['fps_avg']:.2f}\n")
            f.write(f"  ÊúÄÂ§ßFPS: {result['fps_max']:.2f}\n")
            f.write(f"  ÊúÄÂ∞èFPS: {result['fps_min']:.2f}\n")
            f.write(f"  Âπ≥ÂùáÊé®ÁêÜÊó∂Èó¥: {result['avg_time_ms']:.2f}ms\n")
            f.write(f"  ÊúÄÂ∞èÊé®ÁêÜÊó∂Èó¥: {result['min_time_ms']:.2f}ms\n")
            f.write(f"  ÊúÄÂ§ßÊé®ÁêÜÊó∂Èó¥: {result['max_time_ms']:.2f}ms\n")
            f.write(f"  Êó∂Èó¥Ê†áÂáÜÂ∑Æ: {result['std_time_ms']:.2f}ms\n")
            if result['memory_mb'] > 0:
                f.write(f"  ÊòæÂ≠ò‰ΩøÁî®: {result['memory_mb']:.1f}MB\n")
        
        # Êé®ËçêÈÖçÁΩÆ
        if test_results:
            best_result = max(test_results, key=lambda x: x['fps_avg'])
            f.write(f"\nÊé®ËçêÈÖçÁΩÆ:\n")
            f.write(f"  ÊúÄ‰Ω≥Batch Size: {best_result['batch_size']}\n")
            f.write(f"  ÊúÄ‰Ω≥FPS: {best_result['fps_avg']:.2f}\n")
    
    # 2. ‰øùÂ≠òÁÆÄÂçïÁöÑCSVÊ†ºÂºèÊï∞ÊçÆ
    csv_path = os.path.join(log_dir, "fps_results.csv")
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write("Batch_Size,Avg_FPS,Max_FPS,Min_FPS,Avg_Time_ms,Memory_MB\n")
        for result in test_results:
            f.write(f"{result['batch_size']},{result['fps_avg']:.2f},{result['fps_max']:.2f},"
                   f"{result['fps_min']:.2f},{result['avg_time_ms']:.2f},{result['memory_mb']:.1f}\n")
    
    # 3. ÁªòÂà∂FPSÂõæË°®
    plot_fps_charts(test_results, log_dir)
    
    print(f"\nüìÅ FPSÊµãËØïÁªìÊûúÂ∑≤‰øùÂ≠ò:")
    print(f"   ËØ¶ÁªÜÊä•Âëä: {fps_report_path}")
    print(f"   CSVÊï∞ÊçÆ: {csv_path}")
    print(f"   ÂõæË°®: {os.path.join(log_dir, 'fps_chart.png')}")

def plot_fps_charts(test_results, log_dir):
    """ÁªòÂà∂FPSÊÄßËÉΩÂõæË°®"""
    if not test_results:
        return
    
    # ÂáÜÂ§áÊï∞ÊçÆ
    batch_sizes = [r['batch_size'] for r in test_results]
    fps_avg = [r['fps_avg'] for r in test_results]
    fps_max = [r['fps_max'] for r in test_results]
    fps_min = [r['fps_min'] for r in test_results]
    avg_times = [r['avg_time_ms'] for r in test_results]
    
    # ÂàõÂª∫ÂõæË°®
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. FPSÂØπÊØîÂõæ
    ax1.bar(batch_sizes, fps_avg, alpha=0.7, color='skyblue', label='Average FPS')
    ax1.plot(batch_sizes, fps_max, 'ro-', label='Max FPS', markersize=6)
    ax1.plot(batch_sizes, fps_min, 'go-', label='Min FPS', markersize=6)
    ax1.set_xlabel('Batch Size')
    ax1.set_ylabel('FPS')
    ax1.set_title('FPS Performance by Batch Size')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Âú®Êü±Áä∂Âõæ‰∏äÊ∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
    for i, v in enumerate(fps_avg):
        ax1.text(batch_sizes[i], v + 1, f'{v:.1f}', ha='center', va='bottom')
    
    # 2. Êé®ÁêÜÊó∂Èó¥Âõæ
    ax2.plot(batch_sizes, avg_times, 'bo-', linewidth=2, markersize=8)
    ax2.set_xlabel('Batch Size')
    ax2.set_ylabel('Average Inference Time (ms)')
    ax2.set_title('Inference Time by Batch Size')
    ax2.grid(True, alpha=0.3)
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
    for i, v in enumerate(avg_times):
        ax2.text(batch_sizes[i], v, f'{v:.1f}ms', ha='center', va='bottom')
    
    # 3. ÂêûÂêêÈáèÂØπÊØîÔºàÊÄªFPSÔºâ
    total_fps = [r['fps_avg'] for r in test_results]
    ax3.bar(batch_sizes, total_fps, alpha=0.7, color='lightgreen')
    ax3.set_xlabel('Batch Size')
    ax3.set_ylabel('Throughput (Images/Second)')
    ax3.set_title('Model Throughput')
    ax3.grid(True, alpha=0.3)
    
    # 4. ÊòæÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ
    if any(r['memory_mb'] > 0 for r in test_results):
        memory_usage = [r['memory_mb'] for r in test_results]
        ax4.plot(batch_sizes, memory_usage, 'ro-', linewidth=2, markersize=8)
        ax4.set_xlabel('Batch Size')
        ax4.set_ylabel('Memory Usage (MB)')
        ax4.set_title('GPU Memory Usage')
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'GPU Memory\nData Not Available', 
                transform=ax4.transAxes, ha='center', va='center', fontsize=12)
    
    plt.tight_layout()
    
    # ‰øùÂ≠òÂõæË°®
    chart_path = os.path.join(log_dir, 'fps_performance_chart.png')
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # ÂàõÂª∫ÁÆÄÂçïÁöÑFPSÂØπÊØîÂõæ
    plt.figure(figsize=(10, 6))
    plt.bar(batch_sizes, fps_avg, alpha=0.7, color='skyblue', label='Average FPS')
    plt.plot(batch_sizes, fps_max, 'ro-', label='Max FPS', linewidth=2, markersize=8)
    plt.xlabel('Batch Size')
    plt.ylabel('FPS (Frames Per Second)')
    plt.title('Model FPS Performance')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
    for i, v in enumerate(fps_avg):
        plt.text(batch_sizes[i], v + max(fps_avg)*0.02, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')
    
    simple_chart_path = os.path.join(log_dir, 'fps_chart.png')
    plt.savefig(simple_chart_path, dpi=300, bbox_inches='tight')
    plt.close()

# ‰øùÊåÅÂéüÊúâÁöÑfit_one_epochÂáΩÊï∞Ôºå‰ΩÜÁßªÈô§FPSËÆ°ÁÆó
def fit_one_epoch(model_train, model, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, Epoch, cuda, dice_loss, focal_loss, cls_weights, num_classes, fp16, scaler, save_period, save_dir, local_rank):
    total_loss = 0
    val_loss = 0

    if local_rank == 0:
        print('Start Train')
        pbar = tqdm(total=epoch_step,desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)
    
    model_train.train()
    for iteration, batch in enumerate(gen):
        if iteration >= epoch_step:
            break

        imgs, pngs, labels = batch
        with torch.no_grad():
            weights = torch.from_numpy(cls_weights)
            if cuda:
                imgs    = imgs.cuda(local_rank)
                pngs    = pngs.cuda(local_rank)
                labels  = labels.cuda(local_rank)
                weights = weights.cuda(local_rank)

        optimizer.zero_grad()
        if not fp16:
            outputs = model_train(imgs)
            if focal_loss:
                loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)
            else:
                loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)

            if dice_loss:
                main_dice = Dice_loss(outputs, labels)
                loss      = loss + main_dice

            loss.backward()
            optimizer.step()
        else:
            from torch.cuda.amp import autocast
            with autocast():
                outputs = model_train(imgs)
                if focal_loss:
                    loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)
                else:
                    loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)

                if dice_loss:
                    main_dice = Dice_loss(outputs, labels)
                    loss      = loss + main_dice

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

        total_loss += loss.item()
        
        if local_rank == 0:
            pbar.set_postfix(**{'total_loss': total_loss / (iteration + 1), 
                              'lr'        : get_lr(optimizer)})
            pbar.update(1)

    if local_rank == 0:
        pbar.close()
        print('Finish Train')
        print('Start Validation')
        pbar = tqdm(total=epoch_step_val, desc=f'Epoch {epoch + 1}/{Epoch}',postfix=dict,mininterval=0.3)

    model_train.eval()
    for iteration, batch in enumerate(gen_val):
        if iteration >= epoch_step_val:
            break
        imgs, pngs, labels = batch
        with torch.no_grad():
            weights = torch.from_numpy(cls_weights)
            if cuda:
                imgs    = imgs.cuda(local_rank)
                pngs    = pngs.cuda(local_rank)
                labels  = labels.cuda(local_rank)
                weights = weights.cuda(local_rank)

            outputs = model_train(imgs)
            if focal_loss:
                loss = Focal_Loss(outputs, pngs, weights, num_classes = num_classes)
            else:
                loss = CE_Loss(outputs, pngs, weights, num_classes = num_classes)

            if dice_loss:
                main_dice = Dice_loss(outputs, labels)
                loss      = loss + main_dice
        val_loss += loss.item()
        if local_rank == 0:
            pbar.set_postfix(**{'total_loss': val_loss / (iteration + 1)})
            pbar.update(1)
            
    if local_rank == 0:
        pbar.close()
        print('Finish Validation')
        
        print('Epoch:'+ str(epoch + 1) + '/' + str(Epoch))
        print('Total Loss: %.3f || Val Loss: %.3f' % (total_loss / epoch_step, val_loss / epoch_step_val))
        
        #-----------------------------------------------#
        #   ‰øùÂ≠òÊùÉÂÄº
        #-----------------------------------------------#
        if (epoch + 1) % save_period == 0 or epoch + 1 == Epoch:
            torch.save(model.state_dict(), os.path.join(save_dir, f"ep{epoch + 1:03d}-loss{total_loss / epoch_step:.3f}-val_loss{val_loss / epoch_step_val:.3f}.pth"))

        # ÁßªÈô§fpsÂèÇÊï∞
        loss_history.append_loss(epoch + 1, total_loss / epoch_step, val_loss / epoch_step_val)
        eval_callback.on_epoch_end(epoch + 1, model_train)
        print('')
        
        # Âè™ËøîÂõûlossÔºå‰∏çËøîÂõûfps
        return total_loss / epoch_step, val_loss / epoch_step_val
    else:
        return None, None